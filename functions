using DataFrames
# translation of the R code
# starting from the simplest
# all the dots in the names are simply deleted

++++++++++++++++++++++++++++++++++++++++++++
# my additions
type RegOut{V<:Float64}
  β::Matrix{V}
  errors::Matrix{V}
  forecasts::Matrix{V}
end

function OLS (y, x)
  Y = y
  X = x
  β = inv((X'*X))*X'*Y
  return(β)
end

function AR(y, p)
  T = length(y)
  Y = y[(p+1):end, :]
  X = Array(Float64, T-p, p)
  for ii in 1:p
    X[:, ii] = y[(p-ii+1):(T-ii), :]
  end
  β = OLS(Y, X)
  residuals = Y - X*β
  RegOut(β, residuals, zeros(Float64, 10,10))
  # forecasts are not added yet!!
end
# have to figure out how to export these from another file
+++++++++++++++++++++++++++++++++++++++++++++++++
#### TYPE DECLARATIONS
type bvar_data{V<:Float64} # for the bvar_data_work output
  Y::Matrix{V}
  X::Matrix{V}
  Z::Matrix{V}
  Yraw:: Matrix{V}
  p::Int
  m::Int
  T::Int
  k::Int
  constant::Bool
end
+++++++++++++++++++++++++++++++++++++++++++++++++
function vlag(X, p)
    ## This function create
    ## Z = [z_1 z_2 ... z_T]
    ## where
    ## z_t = (1 y_{t-1}' y_{t-2}' ... y_{t-p}')
    n = size(X, 1)
    m = size(X, 2)
    Z = zeros(Float64, n-p, m*p)
    for i=1:p
        @inbounds Z[:, (m*(i-1)+1):(m*i)] = X[(p+1-i):(n-i), :]
    end
    Z
end



function vlag2(X, p)
    ## This function create
    ## Z = [z_1 z_2 ... z_T]
    ## where
    ## z_t = (1 y_{t-1}' y_{t-2}' ... y_{t-p}')
    n = size(X, 1)
    m = size(X, 2)
    Z = zeros(Float64, n-p, m*p)
    for i = 1:p, j = 1:m
        for t = 1:n-p
            @inbounds Z[t, j+(i-1)*m] = X[t+p-i, j]
        end
    end
    Z
end


A = randn(220,100)
vlag(A, 2)-vlag2(A, 2)

@time vlag(A, 10);
@time vlag2(A, 10);

=========================================================

function setbvar (Y, p, h, constant, is.direct = false)
  if isdefined(:h)==false
    h = 1
  else
    if isdefined(:p)==false
      return("lag length should be > 0")
    else

      Yraw = reshape(Y, size(A)[1], size(A)[2])
      Traw = size(Y)[1]
      m = size(Y)[2]
      if is.direct
        Y1 = Yraw[(h+1-1):Traw, :]
        Y2 = Yraw[1:(Traw-h-1), :]
        Traw = Traw - h + 1
      else
        Y1 = Y2 = Yraw
      end

      # construct Y, X, Z
      # X is a (T x k)
      # Z is the block diagonal matrix
      out = DataFrame(out = bvar_data_work(Y1, Y2, Yraw, m, p, Traw, constant))
      out[:Y1] = Y1
      out[:Traw] = Traw
      out[:h] = h
      ##----------------------------OLS-------------------------------
      ## ols = bvar.ols(out[:Y][1], out[:X][1])
      # might get messy to pull the data out of the eventual dataframe,
      # a restructuralization might be good
      ols = NA
      out = DataFrame(ols = ols, data = out, constant = constant)
    end
  end
  out
end

===============================================================

function priorBGR(vardata, lambda, constant, delta, coefsum)
  # PRIORS
  # Construct prior parameters using dummy observations as in BGR
  # equation (5) page 75

  m = vardata[:m][1]
  n = m
  T = vardata[:T][1]
  p = vardata[:p][1]
  k = vardata[:k][1]
  Y = vardata[:Y][1]
  Yraw = vardata[:Yraw][1]
  constant = vardata[:constant][1]

  if isdefined(:delta)==false
    delta = 1
  end
  if length(delta)!=1
    if length(delta)!=m
      return("delta of the wrong dimension")
    else
      delta = rep(delta,m)
    end
  end

  # Get residual variances of univariate p-lag autoregressions
  # Here we just run the AR(p) model on each equation, ignoring the constans
  # and exogenous variables (if they have been specified in the original VAR model)

  # the AR model needed
  s.i = Array(Float64, size(Yraw)[2])
  for i in 1:size(Yraw)[2]
    er = AR(Yraw[:, i], p).errors
    s.i[i] = (er'*er)/length(er)
  end

  # construct Xd
  Jp = diagm(1:p)
  dsl = diagm(sqrt(s.i))/lambda
  dsl2 = diagm(sqrt(s.i)*delta)/lambda
  epsilon = 0.001

  # cbind = hcat
  Xd1 = hcat(kron(Js,dls), zeros(Float64, m*p, 1))
  Xd2 = hcat(zeros(Float64, n, n*p), zeros(Float64, n, 1))
  Xd3 = hcat(zeros(Float64, 1, n*p), epsilon)

  Xd = vcat(Xd1, Xd2, Xd3)
  Yd = vcat(dsl2, zeros(Float64, n*(p-1), n), diagm(sqrt(s.i)), zeros(Float64, 1, n))

  if coefsum == true
    colmeans = Array(Float64, size(vardata.Yraw)[2]) # to be careful with notations of function output (to use types later)
    for i in 1:length(colmeans)
      colmeans[i] = mean(vardata.Yraw[:, i])
    end
    Ydc = diagm(delta*colmeans)/10*lambda
    Xdc = vcat(kron(ones(Float64, p),Ydc), zeros(Float64, m))
    Xd = vcat(Xd, Xdc')
    Yd = vcat(Yd, Ydc)
  end

  Omega0 = inv(Xd'*Xd)
  B0 = inv(Xd'*Xd)*(Xd'*Yd)
  Psi0 = (Yd-Xd*B0)'*(Yd-Xd*B0)
  # rearrange for the constant on top
  # return(list(B = B0, Psi = Psi0, Omega = Omega0, Xd = Xd, Yd = Yd))

end

===============================================================

function bvar_data_work (Y1, Y2, Yraw, m, p, Traw, constant)
  Ylag = vlag2(Y2, p)
  if constant == true
    X1 = hcat(ones(Float64, Traw-p, 1), Ylag)
  else
    X1 = Ylag
  end
  # create the block diagonal matrix Z
  Z = kron(eye(m), X1)
  # form Y matrix accordigly
  # delete first "LAGS" rows to match the dimensions of X matrix
  Y1 = Y1[(p+1):Traw, :]
  # this is the final Y matrix used for the VAR
  # Traw - the dimention of the initial data
  # time series observations of Y and X
  T = Traw - p
  Y = Y1
  X = X1
  k = size(X)[2]
  # return object of interest
  # to think about correct types!!!
  out = bvar_data(Y, X, Z, Yraw, p, m, T, k, constant)
  #out = list(Y = Y, X = X, Z = Z, Yraw = Yraw, p = p, m = m, T = T, k = size(X)[2], constant = constant)
end

=================================================================

function BVARBGR (Y, p, lambda, coefsum = false, constant = true, direct = false, h = 1, predictive = F, ngibbs = 1000, repfor = 10, hmax = 8)
  out = setbvar(Y = Y, p = p, h = h, is.direct = direct, constant = constant)

  #be careful here!!!!
  T = out.data.T
  k = out.data.k
  m = out.data.m
  p = out.data.p

  # setup containers from predictive
  if predictive == true
    if direct == true
      Y_predictive = zeros(Float64, repfor*ngibbs, m)
    else
      Y_predictive = zeros(Float64, repfor*ngibbs, m, hmax)
    end
  else
    Y_predictive = NaN # check the use of NA type
  end

  # FIXME: fix the constant.... at the moment is always included
  prior = priorBGR(out.data, lambda = lambda, constant = constant, delta = 1, coefsum = coefsum)

  # rearrange the data to have the constant at the end
  out.data.X = hcat(out.data.X[:,2:end], ones(Float64, 1, size(out.data.X)[2]))
  # Posterior mean coefficient
  XX = out.data.X'*out.data.X
  XY = out.data.X'*out.data.Y
  Omega = prior.Omega
  iOmega = inv(Omega)

  Ys = vcat(out.data.Y, prior.Yd)
  Xs = vcat(X, prior.Xd)

  # posterior mean of the coefficient
  Bposteriormean = inv(Xs'*Xs)*Xs'*Ys
  bposteriormean = vec(Bposteriormean) # be careful in using vectors

  # fitted mean value
  fitted = out.data.X*B.posterior.mean
  residuals = out.data.Y - fitted
  YY = out.data.Y

  # generate forecast as mean forecast
  if direct == true
    X_fore = [vec(YY[T, :]), vec(X[T, 1:(m*(p-1))]), 1]  # check the dimension of ones
    fcast = X_fore*Bposteriormean
  else
    # generate out-of-sample point forecast
    # 12 period
    fcast = zeros(Float64, 12, m)
    X_fores = zeros(Float64, 12, k)
    X_fore = [vec(YY[T, :]), vec(X[T, 1:(m*(p-1))]), 1]
    for hh in 1:12
      X_fores[hh, :] = X_fore
      fcast[hh, :] = Y_hat = X_fore*Bposteriormean # be careful using vectors
      X_fore = [vec(Y_hat), vec(X_fore[1:(m*(p-1))]), 1]
    end
    # colnames(fcast) = colnames(Y)
    # rownames(fcast) = paste('h=', 1:12, sep='')
  end

  # Draw predictive distribution
  if predictive == true
    X_fore = [YY[T, :], X[T, 1:(m*(p-1))], 1]

    Td = m*p+m+1
    alpha = T+Td+k-2
    # Initialize Matrix SIGMA
    SSE = (Ys-Xs*Bposteriormean)'*(Ys-Xs*Bposteriormean)
    SIGMA = SSE/(T+Ts-k+2)
    cSIGMA = chol(SIGMA)
    # obtain (X'X)^(-1)
    XssX = Xs'*Xs
    iXX = inv((Xs'*Xs))
    # obtain chol ((X'X)^(-1))
    ciXX = chol(iXX)

    ## We can use this this since we will need chol(kron(SIGMA, X'X^(-1))) which
    ## is equal to kron(chol(SIGMA), chol(X'X)^(-1))

    ## Note to myself if R=chol(x), then R'R = x Thus n draws from N(0, V), where
    ## V is (s x s) can be obtained chol(V)%*%matrix(rnorm(n*s), nrow = n) or
    ## crossprod(chol(V), matrix(rnorm(n*s), nrow = n))

    ## Also, draw from a inverse wishart iW(V, nu) can be done by drawing from a
    ## [W(S^(-1),nu)]^(-1)] In code, tcrossprod(crossprod(chol(solve(S)),
    ## matrix(rnorm(nu*s), nrow = s)))


    ## draw_predictive = cSIGMA, ciXX, SSE, m, p, k, direct

    ##out <- draw_predictive(cSIGMA, ciXX, SSE, repfor, ngibbs, h.max, X_fore, alpha, b.posterior.mean, direct)
    for sgibbs in 1:repfor
      # draw B
      V_post = kron(cSIGMA, ciXX)
      bdraw = bposteriormean + V_post'*randn(k*m)
